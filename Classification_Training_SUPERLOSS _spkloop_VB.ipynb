{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functions import *\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.getLogger().addHandler(logging.NullHandler())\n",
    "logging.getLogger(\"natten.functional\").setLevel(logging.ERROR)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "DATASET_PATH = \"../data\"\n",
    "CHECKPOINT_PATH = \"../VIT/EMPR\"\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "wt_dc = 0.06\n",
    "\n",
    "model_type = 'Dinat_Superloss'\n",
    "\n",
    "fourclass = True\n",
    "Speaker_Disentanglement = False\n",
    "Entropy = False\n",
    "pretrain = False\n",
    "pretraining = False\n",
    "new_test = False \n",
    "\n",
    "if pretrain==True:\n",
    "    pathstr = r'/media/carol/Data/Documents/Emo_rec/NewMel/Pretraining/Arousal'\n",
    "    # pathstr = r\"/media/carol/Data/Documents/Emo_rec/Trained Models/DINAT/IEMOCAP/Pre_SD_wtd_006_l\"\n",
    "    \n",
    "    processor_path = os.path.join(pathstr, 'processor')\n",
    "    model_path = os.path.join(pathstr, 'model')\n",
    "    # model_path = os.path.join('G:\\My Drive\\MaSc\\emo_rec\\MODELS\\For Paper\\IEMOCAP\\LABEL_CURRIUCULUM\\PreSD_012', \"model\")\n",
    "elif model_type != 'ViT':\n",
    "    model_path = 'shi-labs/dinat-mini-in1k-224'\n",
    "    processor_path = 'shi-labs/dinat-mini-in1k-224'\n",
    "    pathstr = model_path\n",
    "else:\n",
    "    model_path = 'google/vit-base-patch16-224-in21k'\n",
    "    processor_path = 'google/vit-base-patch16-224-in21k'\n",
    "    pathstr = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = {\n",
    "    0: 'neutral',\n",
    "    1: 'happy',\n",
    "    2: 'sad',\n",
    "    3: 'angry',\n",
    "}\n",
    "Map2Num = {\n",
    "    'neutral': 0,\n",
    "    'happy': 1,\n",
    "    'sad': 2,\n",
    "    'angry': 3,\n",
    "}\n",
    "Cval = 4\n",
    "\n",
    "\n",
    "train_size = 20\n",
    "eval_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer,  SchedulerType\n",
    "\n",
    "metric_name = \"eval_uar\"\n",
    "args = TrainingArguments(\n",
    "    f\"./logs2\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=True,  # This disables the default progress bar\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=SchedulerType.COSINE_WITH_RESTARTS,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=train_size,\n",
    "    per_device_eval_batch_size=eval_size,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=wt_dc,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='./logs_DO_NOT_DELETE/3090',\n",
    "    remove_unused_columns=False,\n",
    "    # logging_strategy=\"epoch\",  # Log at the end of each epoch\n",
    "\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = 'cairocode/IEMO_Mel_6' #'cairocode/IEMO_007_NOSPLIT'\n",
    "train_d0 = load_dataset(dataset_train, split='train')\n",
    "\n",
    "dataset_val = 'cairocode/MSPI_Mel6'#'cairocode/MSPI_007_NOSPLIT_an'\n",
    "val_dataset0 = load_dataset(dataset_val, split='train')\n",
    "val_dataset0  = val_dataset0.filter(filter_m_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tr = os.path.split(dataset_train)[1]\n",
    "ds_vl = os.path.split(dataset_val)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/carol/Data/Documents/Emo_rec/NewMel/IEMO_Mel_6/20250128_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root_path = os.path.join(r'/media/carol/Data/Documents/Emo_rec/NewMel', ds_tr)\n",
    "root_path = create_unique_output_dir(root_path)\n",
    "print(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "NUM = 9 __ SPKRS [10]\n",
      "\n",
      " ########################################################################################################################\n",
      "                                          STARTING SPEAKER 9                                                      \n",
      "\n",
      " ########################################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "Some weights of DinatForImageClassification were not initialized from the model checkpoint at shi-labs/dinat-mini-in1k-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([4, 512]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3160279989242554, 'eval_accuracy': 0.4009216589861751, 'eval_uar': 0.28902101166371635, 'eval_f1': 0.21610827524520387, 'eval_top_k_acc': 0.5944700460829493, 'eval_runtime': 1.6621, 'eval_samples_per_second': 261.119, 'eval_steps_per_second': 13.236, 'epoch': 1.0}\n",
      "{'eval_loss': 1.1933103799819946, 'eval_accuracy': 0.49539170506912444, 'eval_uar': 0.39946404586945894, 'eval_f1': 0.363358123569794, 'eval_top_k_acc': 0.7534562211981567, 'eval_runtime': 1.6496, 'eval_samples_per_second': 263.095, 'eval_steps_per_second': 13.337, 'epoch': 2.0}\n",
      "{'loss': 1.2607, 'grad_norm': 37.41012954711914, 'learning_rate': 4.926108374384237e-06, 'epoch': 2.4630541871921183}\n",
      "{'eval_loss': 1.1048977375030518, 'eval_accuracy': 0.5506912442396313, 'eval_uar': 0.4706426049002358, 'eval_f1': 0.4654629535733865, 'eval_top_k_acc': 0.7903225806451613, 'eval_runtime': 1.6902, 'eval_samples_per_second': 256.771, 'eval_steps_per_second': 13.016, 'epoch': 3.0}\n",
      "{'eval_loss': 1.0351556539535522, 'eval_accuracy': 0.5092165898617511, 'eval_uar': 0.5089632018435452, 'eval_f1': 0.5039789929014328, 'eval_top_k_acc': 0.7764976958525346, 'eval_runtime': 1.6567, 'eval_samples_per_second': 261.967, 'eval_steps_per_second': 13.279, 'epoch': 4.0}\n",
      "{'loss': 1.0332, 'grad_norm': 41.316890716552734, 'learning_rate': 9.852216748768475e-06, 'epoch': 4.926108374384237}\n",
      "{'eval_loss': 1.0514994859695435, 'eval_accuracy': 0.5506912442396313, 'eval_uar': 0.5477319785499607, 'eval_f1': 0.5192059131397923, 'eval_top_k_acc': 0.8018433179723502, 'eval_runtime': 1.6598, 'eval_samples_per_second': 261.476, 'eval_steps_per_second': 13.255, 'epoch': 5.0}\n",
      "{'eval_loss': 1.0072473287582397, 'eval_accuracy': 0.5645161290322581, 'eval_uar': 0.5349260428507224, 'eval_f1': 0.5422393076500253, 'eval_top_k_acc': 0.8202764976958525, 'eval_runtime': 1.6839, 'eval_samples_per_second': 257.741, 'eval_steps_per_second': 13.065, 'epoch': 6.0}\n",
      "{'eval_loss': 1.1613388061523438, 'eval_accuracy': 0.543778801843318, 'eval_uar': 0.485315673882035, 'eval_f1': 0.4676041494022906, 'eval_top_k_acc': 0.804147465437788, 'eval_runtime': 1.6892, 'eval_samples_per_second': 256.92, 'eval_steps_per_second': 13.024, 'epoch': 7.0}\n",
      "{'loss': 0.9247, 'grad_norm': 19.598169326782227, 'learning_rate': 9.93060965701654e-06, 'epoch': 7.389162561576355}\n",
      "{'eval_loss': 1.0192179679870605, 'eval_accuracy': 0.5852534562211982, 'eval_uar': 0.5552947488419366, 'eval_f1': 0.5531210590660521, 'eval_top_k_acc': 0.8317972350230415, 'eval_runtime': 1.7676, 'eval_samples_per_second': 245.524, 'eval_steps_per_second': 12.446, 'epoch': 8.0}\n",
      "{'eval_loss': 1.122564673423767, 'eval_accuracy': 0.5576036866359447, 'eval_uar': 0.5026056425451721, 'eval_f1': 0.4992174881193573, 'eval_top_k_acc': 0.7857142857142857, 'eval_runtime': 1.6522, 'eval_samples_per_second': 262.684, 'eval_steps_per_second': 13.316, 'epoch': 9.0}\n",
      "{'loss': 0.8293, 'grad_norm': 30.52958106994629, 'learning_rate': 9.715856209871243e-06, 'epoch': 9.852216748768473}\n",
      "{'eval_loss': 1.0808054208755493, 'eval_accuracy': 0.5599078341013825, 'eval_uar': 0.5151268326063316, 'eval_f1': 0.5163924820354858, 'eval_top_k_acc': 0.8202764976958525, 'eval_runtime': 1.6468, 'eval_samples_per_second': 263.536, 'eval_steps_per_second': 13.359, 'epoch': 10.0}\n",
      "{'eval_loss': 1.1709569692611694, 'eval_accuracy': 0.543778801843318, 'eval_uar': 0.49737003703356386, 'eval_f1': 0.4896061790184465, 'eval_top_k_acc': 0.8110599078341014, 'eval_runtime': 1.6501, 'eval_samples_per_second': 263.021, 'eval_steps_per_second': 13.333, 'epoch': 11.0}\n",
      "{'eval_loss': 1.1428827047348022, 'eval_accuracy': 0.532258064516129, 'eval_uar': 0.5026079807033194, 'eval_f1': 0.5021023360262156, 'eval_top_k_acc': 0.7949308755760369, 'eval_runtime': 1.6767, 'eval_samples_per_second': 258.845, 'eval_steps_per_second': 13.121, 'epoch': 12.0}\n",
      "{'loss': 0.7098, 'grad_norm': 45.928741455078125, 'learning_rate': 9.36200734016203e-06, 'epoch': 12.31527093596059}\n",
      "{'eval_loss': 1.2601701021194458, 'eval_accuracy': 0.5460829493087558, 'eval_uar': 0.5225923703905737, 'eval_f1': 0.5113616573786304, 'eval_top_k_acc': 0.815668202764977, 'eval_runtime': 1.6645, 'eval_samples_per_second': 260.733, 'eval_steps_per_second': 13.217, 'epoch': 13.0}\n",
      "{'eval_loss': 1.2255218029022217, 'eval_accuracy': 0.5529953917050692, 'eval_uar': 0.5153047516881413, 'eval_f1': 0.5108710163022202, 'eval_top_k_acc': 0.8087557603686636, 'eval_runtime': 1.6521, 'eval_samples_per_second': 262.693, 'eval_steps_per_second': 13.316, 'epoch': 14.0}\n",
      "{'loss': 0.6052, 'grad_norm': 33.576107025146484, 'learning_rate': 8.879499913617107e-06, 'epoch': 14.77832512315271}\n",
      "{'eval_loss': 1.4077688455581665, 'eval_accuracy': 0.5668202764976958, 'eval_uar': 0.5045591401472144, 'eval_f1': 0.5057641492602198, 'eval_top_k_acc': 0.815668202764977, 'eval_runtime': 1.6354, 'eval_samples_per_second': 265.38, 'eval_steps_per_second': 13.452, 'epoch': 15.0}\n",
      "{'eval_loss': 1.4218881130218506, 'eval_accuracy': 0.5645161290322581, 'eval_uar': 0.5207879381940843, 'eval_f1': 0.514641059300688, 'eval_top_k_acc': 0.8087557603686636, 'eval_runtime': 1.6347, 'eval_samples_per_second': 265.497, 'eval_steps_per_second': 13.458, 'epoch': 16.0}\n",
      "{'eval_loss': 1.4780933856964111, 'eval_accuracy': 0.5552995391705069, 'eval_uar': 0.51014469595575, 'eval_f1': 0.5004137115839243, 'eval_top_k_acc': 0.8133640552995391, 'eval_runtime': 1.6445, 'eval_samples_per_second': 263.905, 'eval_steps_per_second': 13.378, 'epoch': 17.0}\n",
      "{'loss': 0.4961, 'grad_norm': 42.35152816772461, 'learning_rate': 8.282565614028068e-06, 'epoch': 17.24137931034483}\n",
      "{'eval_loss': 1.6266711950302124, 'eval_accuracy': 0.5599078341013825, 'eval_uar': 0.5057470630766577, 'eval_f1': 0.4933445092810759, 'eval_top_k_acc': 0.8018433179723502, 'eval_runtime': 1.652, 'eval_samples_per_second': 262.718, 'eval_steps_per_second': 13.318, 'epoch': 18.0}\n",
      "{'eval_loss': 1.852723240852356, 'eval_accuracy': 0.5345622119815668, 'eval_uar': 0.4924268443516644, 'eval_f1': 0.47360224113662674, 'eval_top_k_acc': 0.771889400921659, 'eval_runtime': 1.7869, 'eval_samples_per_second': 242.877, 'eval_steps_per_second': 12.312, 'epoch': 19.0}\n",
      "{'loss': 0.4132, 'grad_norm': 100.6751480102539, 'learning_rate': 7.588811175983305e-06, 'epoch': 19.704433497536947}\n",
      "{'eval_loss': 1.6395443677902222, 'eval_accuracy': 0.5737327188940092, 'eval_uar': 0.5304211765847848, 'eval_f1': 0.5309968024757066, 'eval_top_k_acc': 0.8064516129032258, 'eval_runtime': 1.8387, 'eval_samples_per_second': 236.041, 'eval_steps_per_second': 11.965, 'epoch': 20.0}\n",
      "{'train_runtime': 968.5457, 'train_samples_per_second': 209.386, 'train_steps_per_second': 10.48, 'train_loss': 0.7781341303745514, 'epoch': 20.0}\n",
      "58.525345622119815 \t 55.52947488419366\n",
      "{'test_loss': 1.0192179679870605, 'test_accuracy': 0.5852534562211982, 'test_uar': 0.5552947488419366, 'test_f1': 0.5531210590660521, 'test_top_k_acc': 0.8317972350230415, 'test_runtime': 1.7351, 'test_samples_per_second': 250.135, 'test_steps_per_second': 12.68}\n",
      "File saved successfully at: /media/carol/Data/Documents/Emo_rec/NewMel/IEMO_Mel_6/20250128_2/9/header.txt\n",
      "Confusion matrix saved to: /media/carol/Data/Documents/Emo_rec/NewMel/IEMO_Mel_6/20250128_2/9/results/IEMO_Mel_6_accuracy_58.53_UAR_55.53.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 154\u001b[0m\n\u001b[1;32m    152\u001b[0m matrix_path \u001b[38;5;241m=\u001b[39m save_confusion_matrix(outputs, dataset_train, new_model_path, Map2Num)\n\u001b[1;32m    153\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m dataset_val\n\u001b[0;32m--> 154\u001b[0m outputs2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# print(outputs2.metrics)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# print(outputs2.metrics['test_accuracy']*100, \"\\t\", outputs2.metrics['test_uar']*100)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m new_row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_tr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ACC\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_tr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_UAR\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_uar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_vl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ACC\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs2\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_vl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_UAR\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs2\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_uar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m}\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/transformers/trainer.py:3543\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3540\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3542\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3543\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3546\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/transformers/trainer.py:3640\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3637\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3639\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3640\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3641\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3642\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2784\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2784\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2780\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2765\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2764\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2765\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2767\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:521\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    520\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m--> 521\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(batch)\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:228\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/features/features.py:2088\u001b[0m, in \u001b[0;36mFeatures.decode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2085\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2087\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2088\u001b[0m         [\n\u001b[1;32m   2089\u001b[0m             decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   2090\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2093\u001b[0m         ]\n\u001b[1;32m   2094\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2096\u001b[0m     )\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/features/features.py:2089\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2085\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2087\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2088\u001b[0m         [\n\u001b[0;32m-> 2089\u001b[0m             \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2090\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2091\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   2093\u001b[0m         ]\n\u001b[1;32m   2094\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   2095\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   2096\u001b[0m     )\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/features/features.py:1404\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode_example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m-> 1404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/datasets/features/image.py:188\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[0;32m--> 188\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetexif()\u001b[38;5;241m.\u001b[39mget(PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n",
      "File \u001b[0;32m/media/carol/Data/Documents/Emo_rec/.venv/lib/python3.10/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from collections import defaultdict\n",
    "# Example usage\n",
    "\n",
    "log_file = os.path.join(root_path,\"training_logs.csv\")\n",
    "fieldnames = [\"timestamp\", \"log\"]\n",
    "sys.stdout = CSVLogger(log_file, fieldnames)\n",
    "\n",
    "xcorp_results = {\n",
    "    'y_true': [],\n",
    "    'y_pred': {f'run_{i}': [] for i in range(10)}\n",
    "}\n",
    "spkrs = [sample['speakerID'] for sample in train_d0]\n",
    "unique_speakers = list(set(spkrs))\n",
    "print(unique_speakers)\n",
    "\n",
    "all_y_true = defaultdict(list)\n",
    "all_y_pred = defaultdict(list)\n",
    "\n",
    "angry_weight = 1\n",
    "happy_weight = 5\n",
    "neutral_weight = 2\n",
    "sad_weight = 1\n",
    "\n",
    "cw_dict = {}\n",
    "cw_dict[0] = [1.1,1.5,1,1] #CONFIRMED\n",
    "cw_dict[1] = [1.1,1.5,1,1] #CONFIRMED\n",
    "\n",
    "cw_dict[2] = [0.97,1.5,1,1]\n",
    "cw_dict[3] = [1,1.6,1,1]  #CONFIRMED\n",
    "\n",
    "cw_dict[4] = [0.97,1.7,1,1]\n",
    "\n",
    "cw_dict[5] = [0.95,1.7,1,1]\n",
    "\n",
    "cw_dict[6] = [1.05,1.7,1,1]\n",
    "\n",
    "cw_dict[7] = [1,1.8,0.93,1] #CONFIRMED\n",
    "cw_dict[8] = [1,1.5,0.92,1] \n",
    "\n",
    "cw_dict[9] = [0.95,2.5,0.95,1] \n",
    "\n",
    "\n",
    "# class_weights = [2, 6.5,1,1]\n",
    "\n",
    "class_weight_multipliers = {\n",
    "    0: neutral_weight,\n",
    "    1: happy_weight,\n",
    "    2: sad_weight,\n",
    "    3: angry_weight\n",
    "}\n",
    "total_results = pd.DataFrame()\n",
    "\n",
    "for i in range (len(unique_speakers)):\n",
    "    \n",
    "    # speakers = [unique_speakers[i]]  #    speakers = [937+i]\n",
    "    speakers = [11-unique_speakers[i]]\n",
    "\n",
    "    num = speakers[0]-1\n",
    "    print(f\"NUM = {num} __ SPKRS {speakers}\")\n",
    "    print(f\"\\n {'#'*120}\")\n",
    "    print(f\"                                          STARTING SPEAKER {num}                                                      \")\n",
    "    print(f\"\\n {'#'*120}\")\n",
    "\n",
    "    new_model_path = os.path.join(root_path, str(num))\n",
    "    os.makedirs(new_model_path, exist_ok=True)\n",
    "    \n",
    "    # Create the test split\n",
    "    test_dataset = train_d0.filter(lambda x: x['speakerID'] in speakers).filter(filter_m_examples)\n",
    "    \n",
    "    # Create the remaining data\n",
    "    train_set = train_d0.filter(lambda x: x['speakerID'] not in speakers).filter(filter_m_examples)\n",
    "\n",
    "    # train_set = train_set.train_test_split(test_size = 0.2)\n",
    "    train_dataset = train_set #['train']\n",
    "    val_dataset = test_dataset #train_set['test']\n",
    "    # val_dataset = concatenate_datasets([val_dataset0, test_dataset])\n",
    "\n",
    "    processor = AutoImageProcessor.from_pretrained(processor_path)\n",
    "\n",
    "    base_model = DinatForImageClassification.from_pretrained(\n",
    "        model_path,\n",
    "        id2label=EMOTIONS,\n",
    "        num_labels=Cval,\n",
    "        label2id=Map2Num,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        problem_type='single_label_classification'\n",
    "    )\n",
    "    if Speaker_Disentanglement == True:\n",
    "        custom_dataset = CustomDataset(train_dataset)\n",
    "        custom_sampler = CustomSampler(custom_dataset)\n",
    "        train_dataloader = DataLoader(train_dataset, sampler=custom_sampler, collate_fn=collate_fn, batch_size=train_size)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=train_size)\n",
    "\n",
    "    # class_weights = calculate_class_weights(train_dataset, class_weight_multipliers)\n",
    "    # class_weights = [1.1,1.8,0.93,1]\n",
    "    # class_weights = [neutral]\n",
    "    class_weights = cw_dict[num]\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    model = CustomDinatForImageClassification(base_model, num_classes=Cval, class_weights= class_weights)#, initial_class_weights=class_weights)\n",
    "\n",
    "    model.to(device)\n",
    "    super_loss_params = {\n",
    "    'C': Cval,  # Example value, adjust based on your needs\n",
    "    'lam': 0.01,  # Example value, adjust based on your needs\n",
    "    'batch_size': args.train_batch_size,  # Pass the batch size dynamically\n",
    "    'class_weights':class_weights\n",
    "    }\n",
    "\n",
    "    \n",
    "    val_dataset.set_transform(val_transforms)\n",
    "    val_dataset0.set_transform(val_transforms)\n",
    "    train_dataset.set_transform(train_transforms)\n",
    "\n",
    "    # early_stopping = EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.001)\n",
    "    early_stopping = EarlyStoppingCallback(early_stopping_patience=12, early_stopping_threshold=0.001)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=processor,\n",
    "        callbacks=[early_stopping]#, ClassWeightLoggerCallback()],  # Add the new callback here\n",
    "        # super_loss_params=super_loss_params,  # Pass the custom loss parameters here\n",
    "        )\n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(new_model_path, \"model\"))\n",
    "    processor.save_pretrained(os.path.join(new_model_path, \"processor\"))\n",
    "    test_dataset.set_transform(test_transforms)\n",
    "    outputs = trainer.predict(test_dataset)\n",
    "    print(outputs.metrics['test_accuracy']*100, \"\\t\", outputs.metrics['test_uar']*100)\n",
    "    print(outputs.metrics)\n",
    "    model_info = {\n",
    "        \"Pretrain_file\": pathstr,\n",
    "        \"Dataset Used\": dataset_train,\n",
    "        \"Model Type\": model_type,\n",
    "        \"Super Loss PARAMS\": super_loss_params,\n",
    "        \"Speaker Disentanglement\": Speaker_Disentanglement,\n",
    "        \"Entropy Curriculum Training\": Entropy,\n",
    "        \"Column Trained on\": column,\n",
    "        \"Test Results\": outputs.metrics,\n",
    "        \"Test SpeakerID\": speakers,\n",
    "        \"Class Weight\": cw_dict[num],\n",
    "        \"NUM\":  num,\n",
    "        \"Weight decay\": wt_dc\n",
    "    }\n",
    "    file_path = save_model_header(new_model_path, model_info)\n",
    "    matrix_path = save_confusion_matrix(outputs, dataset_train, new_model_path, Map2Num)\n",
    "    dataset_test = dataset_val\n",
    "    outputs2 = trainer.predict(val_dataset0)\n",
    "    # print(outputs2.metrics)\n",
    "    # print(outputs2.metrics['test_accuracy']*100, \"\\t\", outputs2.metrics['test_uar']*100)\n",
    "\n",
    "    new_row = {f'{ds_tr}_ACC': outputs.metrics['test_accuracy']*100, f'{ds_tr}_UAR': outputs.metrics['test_uar']*100, f'{ds_vl}_ACC': outputs2.metrics['test_accuracy']*100,f'{ds_vl}_UAR': outputs2.metrics['test_uar']*100}\n",
    "    print(f\"\\n {'-'*120}\")\n",
    "    print(\"\\n\\n\", new_row, \"\\n\")\n",
    "    \n",
    "    # Method 1: Using concat()\n",
    "    total_results = pd.concat([total_results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    if i == 0:\n",
    "        xcorp_results['y_true'].extend(outputs2.label_ids)\n",
    "\n",
    "    # Store y_pred for each run\n",
    "    xcorp_results['y_pred'][f'run_{i}'].extend(outputs2.predictions.argmax(axis=1))\n",
    "\n",
    "\n",
    "    matrix_path = save_confusion_matrix(outputs2, dataset_test, new_model_path, Map2Num)\n",
    "    all_y_true[ds_tr].extend(outputs.label_ids)\n",
    "    all_y_pred[ds_tr].extend(outputs.predictions.argmax(axis=1))\n",
    "    all_y_true[ds_vl].extend(outputs2.label_ids)\n",
    "    all_y_pred[ds_vl].extend(outputs2.predictions.argmax(axis=1))\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    del trainer\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 1.5000, 1.0000, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 1.5000, 1.0000, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1000, 1.5000, 1.0000, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to get the final prediction based on majority voting across all runs\n",
    "y_true = np.array(xcorp_results['y_true'])\n",
    "\n",
    "final_prediction = np.array([np.bincount([xcorp_results['y_pred'][f'run_{i}'][j] for i in range(10)]).argmax() \n",
    "                             for j in range(len(y_true))])\n",
    "\n",
    "final_accuracy = accuracy_score(y_true, final_prediction)\n",
    "print(f\"Final accuracy after majority voting: {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_results)\n",
    "avg_results = total_results.mean(numeric_only=True)\n",
    "\n",
    "print(\"\\nAVERAGE RESULTS\\n\", avg_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to: /media/carol/Data/Documents/Emo_rec/NewMel/IEMO_Mel_6/20250127_8/0/results/MSPI_Mel6_accuracy_41.60_UAR_42.01.png\n",
      "Final accuracy after majority voting: 0.4202359579379328\n",
      "   IEMO_Mel_6_ACC  IEMO_Mel_6_UAR  MSPI_Mel6_ACC  MSPI_Mel6_UAR\n",
      "0       57.834101       57.032616      41.677353      41.441634\n",
      "1       54.330709       56.816101      44.870480      40.437648\n",
      "2       63.007160       60.673376      38.766350      42.550904\n",
      "3       66.310160       56.423506      41.959477      41.289640\n",
      "4       58.174905       50.375917      32.110798      39.425246\n",
      "5       58.438819       55.290187      39.907669      41.631812\n",
      "6       58.411215       62.652935      38.163632      44.243978\n",
      "7       59.220779       57.054510      40.228264      42.044744\n",
      "8       59.538784       62.884004      39.381893      43.156237\n",
      "9       61.720430       62.722147      41.600410      42.005566\n",
      "\n",
      "AVERAGE RESULTS\n",
      " IEMO_Mel_6_ACC    59.698706\n",
      "IEMO_Mel_6_UAR    58.192530\n",
      "MSPI_Mel6_ACC     39.866632\n",
      "MSPI_Mel6_UAR     41.822741\n",
      "dtype: float64\n",
      "\n",
      "Final Metrics:\n",
      "{'IEMO_Mel_6_ACC': 59.487750556792875, 'IEMO_Mel_6_UAR': 57.59198129630061, 'MSPI_Mel6_ACC': 39.866632469864065, 'MSPI_Mel6_UAR': 41.82274099630755}\n",
      "File saved successfully at: /media/carol/Data/Documents/Emo_rec/NewMel/IEMO_Mel_6/20250127_8/header.txt\n"
     ]
    }
   ],
   "source": [
    "final_metrics = {}\n",
    "for dataset in [ds_tr, ds_vl]:\n",
    "    y_true = np.array(all_y_true[dataset])\n",
    "    y_pred = np.array(all_y_pred[dataset])\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred) * 100\n",
    "    uar = balanced_accuracy_score(y_true, y_pred) * 100\n",
    "    \n",
    "    final_metrics[f'{dataset}_ACC'] = acc\n",
    "    final_metrics[f'{dataset}_UAR'] = uar\n",
    "\n",
    "print(\"\\nFinal Metrics:\")\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"Pretrain_file\": pathstr,\n",
    "    \"Dataset Used\": dataset_train,\n",
    "    \"Model Type\": model_type,\n",
    "    \"Super Loss PARAMS\": super_loss_params,\n",
    "    \"Speaker Disentanglement\": Speaker_Disentanglement,\n",
    "    \"Entropy Curriculum Training\": Entropy,\n",
    "    \"Column Trained on\": column,\n",
    "    \"Test SpeakerID\": speakers,\n",
    "    \"Angry Weight\": angry_weight,\n",
    "    \"Happy Weight\": happy_weight,\n",
    "    \"Neutral Weight\": neutral_weight,\n",
    "    \"Sad Weight\": sad_weight,\n",
    "    \"Weight decay\": wt_dc, \n",
    "    \"Avg Results\": avg_results, \n",
    "    \"total results\": final_metrics\n",
    "}\n",
    "file_path = save_model_header(root_path, model_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
